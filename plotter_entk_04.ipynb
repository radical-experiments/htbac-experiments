{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "from decimal import *\n",
    "getcontext().prec = 6\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trials = 1    # Number of trials for each data point \n",
    "cores = [64, 128, 256, 512, 1024]      # List of number of total cores at various data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_exec_time(df_cu):\n",
    "    \n",
    "    # Convert dataframe into a dictionary - for each uid (compute unit id), we get\n",
    "    # the timestamps for 'Executing' and 'AgentStagningOutputPending' states\n",
    "    \n",
    "    '''\n",
    "    The structure of the dictionary is as below:\n",
    "    super_dict = {\n",
    "                    'unit.00000': {\n",
    "                                    'Executing': timestamp1,\n",
    "                                    'AgentStagingOutputPending': timestamp2\n",
    "                            }\n",
    "                }\n",
    "    '''\n",
    "    \n",
    "    super_dict = dict()\n",
    "    for row in df_cu.iterrows():\n",
    "        row=row[1]\n",
    "        uid = row['uid']\n",
    "        start_probe = float(row['Executing'])\n",
    "        end_probe = float(row['AgentStagingOutputPending'])\n",
    "\n",
    "        if uid not in super_dict:\n",
    "            super_dict[uid] = dict()\n",
    "            \n",
    "        if 'Executing' not in super_dict[uid]:\n",
    "            super_dict[uid]['Executing'] = start_probe\n",
    "            \n",
    "        if 'AgentStagingOutputPending' not in super_dict[uid]:\n",
    "            super_dict[uid]['AgentStagingOutputPending'] = end_probe\n",
    "            \n",
    "\n",
    "    # Use the magic function to get the total time spent between 'Executing' and 'AgentStagingOutputPending'\n",
    "    return get_Toverlap(super_dict, 'Executing', 'AgentStagingOutputPending')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data_df, err_df):\n",
    "    \n",
    "    # Fontsize for all text in the plot\n",
    "    FONTSIZE=18\n",
    "    \n",
    "    ov_data = data_df['EnTK overhead']\n",
    "    ov_err = err_df['EnTK overhead']\n",
    "    \n",
    "    exec_data = data_df['RP overhead']\n",
    "    exec_err = err_df['RP overhead']\n",
    "    \n",
    "    ov_ax = ov_data.plot(kind='bar', \n",
    "                         yerr=ov_err, \n",
    "                         position=1, \n",
    "                         width=0.15, \n",
    "                         color='red',\n",
    "                         fontsize=FONTSIZE)\n",
    "    \n",
    "    exec_ax = ov_ax.twinx()\n",
    "    \n",
    "    exec_ax = exec_data.plot(ax=exec_ax, \n",
    "                             kind='bar', \n",
    "                             yerr=exec_err, \n",
    "                             secondary_y=True, \n",
    "                             position=0, \n",
    "                             width=0.15, \n",
    "                             color='blue',\n",
    "                            fontsize=FONTSIZE)\n",
    "    \n",
    "    ov_ax.set_xlabel('Total number of pipelines', fontsize=FONTSIZE)\n",
    "    ov_ax.set_ylabel('EnTK Overhead (seconds)', fontsize=FONTSIZE)\n",
    "    ov_ax.set_ylim(0,10)\n",
    "\n",
    "    exec_ax.set_ylabel('RP Overhead (seconds)', fontsize=FONTSIZE)\n",
    "    exec_ax.set_ylim(0,1000)\n",
    "       \n",
    "    # TODO: Following two lines does not make an effect, have to check how to assign the fontsize\n",
    "    # for the tick labels of the seconday y axis\n",
    "    for tick in exec_ax.yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(FONTSIZE) \n",
    "    \n",
    "    \n",
    "    l1, h1 = ov_ax.get_legend_handles_labels()\n",
    "    l2, h2 = exec_ax.get_legend_handles_labels()\n",
    "    \n",
    "    l1.extend(l2)\n",
    "    h1.extend(h2)\n",
    "    \n",
    "    ov_ax.legend(l1, h1, fontsize=FONTSIZE)\n",
    "    ov_ax.set_title('Weak scaling behaviour of HT-BAC workflow on NCSA Blue Waters \\n with null workload varying number of pipelines, number of cores per pipeline = 8')    \n",
    "    plt.savefig('weak_scaling_null_workload_bw.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Magic function\n",
    "\n",
    "def get_Toverlap(d, start_state, stop_state):\n",
    "    '''\n",
    "    Helper function to create the list of lists from which to calculate the\n",
    "    overlap of the elements of a DataFrame between the two boundaries passed as\n",
    "     arguments.\n",
    "    '''\n",
    "\n",
    "    overlap = 0\n",
    "    ranges = []\n",
    "\n",
    "    for obj, states in d.iteritems():\n",
    "        #print states\n",
    "        ranges.append([states[start_state], states[stop_state]])\n",
    "\n",
    "    for crange in collapse_ranges(ranges):\n",
    "        overlap += crange[1] - crange[0]\n",
    "    \n",
    "    return overlap\n",
    "\n",
    "def collapse_ranges(ranges):\n",
    "    \"\"\"\n",
    "    given be a set of ranges (as a set of pairs of floats [start, end] with\n",
    "    'start <= end'. This algorithm will then collapse that set into the\n",
    "    smallest possible set of ranges which cover the same, but not more nor\n",
    "    less, of the domain (floats).\n",
    "    \n",
    "    We first sort the ranges by their starting point. We then start with the\n",
    "    range with the smallest starting point [start_1, end_1], and compare to the\n",
    "    next following range [start_2, end_2], where we now know that start_1 <=\n",
    "    start_2. We have now two cases:\n",
    "    \n",
    "    a) when start_2 <= end_1, then the ranges overlap, and we collapse them\n",
    "    into range_1: range_1 = [start_1, max[end_1, end_2]\n",
    "    \n",
    "    b) when start_2 > end_2, then ranges don't overlap. Importantly, none of\n",
    "    the other later ranges can ever overlap range_1. So we move range_1 to\n",
    "    the set of final ranges, and restart the algorithm with range_2 being\n",
    "    the smallest one.\n",
    "    \n",
    "    Termination condition is if only one range is left -- it is also moved to\n",
    "    the list of final ranges then, and that list is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    final = []\n",
    "\n",
    "    # sort ranges into a copy list\n",
    "    _ranges = sorted (ranges, key=lambda x: x[0])\n",
    "        \n",
    "    START = 0\n",
    "    END = 1\n",
    "\n",
    "    base = _ranges[0] # smallest range\n",
    "\n",
    "    for _range in _ranges[1:]:\n",
    "\n",
    "        if _range[START] <= base[END]:\n",
    "            # ranges overlap -- extend the base\n",
    "            base[END] = max(base[END], _range[END])\n",
    "\n",
    "        else:\n",
    "\n",
    "            # ranges don't overlap -- move base to final, and current _range\n",
    "            # becomes the new base\n",
    "            final.append(base)\n",
    "            base = _range\n",
    "\n",
    "    # termination: push last base to final\n",
    "    final.append(base)\n",
    "\n",
    "    return final   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entk_overhead(df_pat):\n",
    "    \n",
    "    \n",
    "    # Convert dataframe into a dictionary - for each task (we get a unique name by using the stage and \n",
    "    # the pipeline number in combination), we get\n",
    "    # the timestamps for 'start_time', 'wait_time', 'res_time' and 'done_time' events\n",
    "    \n",
    "    '''\n",
    "    The structure of the dictionary is as below:\n",
    "    super_dict = {\n",
    "                    'stage1-pipeline1': {\n",
    "                                    'start_time': timestamp1, \n",
    "                                    'wait_time': timestamp2, \n",
    "                                    'res_time': timestamp3, \n",
    "                                    'done_time': timestamp4\n",
    "                            }\n",
    "                }\n",
    "    '''\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "\n",
    "    def totimestamp(dt, epoch=datetime(1970,1,1)):\n",
    "        td = dt - epoch\n",
    "        #return td.total_seconds()\n",
    "        return (td.microseconds + (td.seconds + td.days * 86400) * 10**6) \n",
    "\n",
    "    \n",
    "    super_dict = dict()\n",
    "    for row in df_pat.iterrows():\n",
    "        row=row[1]\n",
    "        stage = row['stage']\n",
    "        pipeline = row['pipeline']\n",
    "        probe = row['probe']\n",
    "        #timestamp = time.mktime(time.strptime(row['timestamp'], \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "        #print datetime.strptime(row['timestamp'], \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        precise_epoch = totimestamp(datetime.strptime(row['timestamp'], \"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "        # totimestamp(datetime.fromtimestamp(timestamp))\n",
    "        #print timestamp, precise_epoch\n",
    "        if '%s-%s'%(stage, pipeline) not in super_dict:\n",
    "            super_dict['%s-%s'%(stage, pipeline)] = dict()\n",
    "            \n",
    "        if probe not in super_dict['%s-%s'%(stage, pipeline)]:\n",
    "            super_dict['%s-%s'%(stage, pipeline)][probe] = precise_epoch\n",
    "            \n",
    "    return (get_Toverlap(super_dict, 'start_time', 'wait_time') + get_Toverlap(super_dict, 'res_time', 'done_time'))/ 10**6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#MAIN\n",
    "\n",
    "data_df = pd.DataFrame(columns=['EnTK overhead',\n",
    "                                'RP overhead'])\n",
    "\n",
    "err_df = pd.DataFrame(columns=[ 'EnTK overhead',\n",
    "                                'RP overhead'])\n",
    "\n",
    "for c in cores:\n",
    "    \n",
    "    \n",
    "    # This list is to get the average and error for each result across multiple trials\n",
    "    entk_ov_list = list()\n",
    "    exec_list = list()\n",
    "    \n",
    "    for t in range(1,trials+1):\n",
    "        \n",
    "        # Read csv files into DataFrames\n",
    "        df_pat = pd.read_csv('weak_scaling_null_workload_data/null-ws-{1}cores-trial{0}/enmd_pat_overhead.csv'.format(t,c),\n",
    "                             header=0,\n",
    "                             sep=',',\n",
    "                             skipinitialspace=True)\n",
    "        fname = glob.glob('weak_scaling_null_workload_data/null-ws-{1}cores-trial{0}/execution*.csv'.format(t,c))[0]\n",
    "        df_cu = pd.read_csv('{0}'.format(fname),header=0,sep=',',skipinitialspace=True)\n",
    "        \n",
    "        \n",
    "        # Methods to get the EnTK overhead, execution time\n",
    "        entk_ov = extract_entk_overhead(df_pat)\n",
    "        exec_time = extract_exec_time(df_cu)\n",
    "        \n",
    "        \n",
    "        # Add overhead and execution time from current iteration to list\n",
    "        entk_ov_list.append(entk_ov)\n",
    "        exec_list.append(exec_time)\n",
    " \n",
    "    # Get the average and stderr when we have done multiple trials for each data point\n",
    "    data_df.loc[c/8] = [np.average(entk_ov_list),\n",
    "                        np.average(exec_list)]\n",
    "    \n",
    "    err_df.loc[c/8] = [ np.std(entk_ov_list)/math.sqrt(trials),\n",
    "                    np.std(exec_list)/math.sqrt(trials)]\n",
    "\n",
    "# Bar plot of the results    \n",
    "#print data_df\n",
    "plot(data_df,err_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
